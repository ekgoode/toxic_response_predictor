# toxic_response_predictor
In this repository I explore the question, "Can LLMs predict whether a human engineered prompt is likely to return a toxic response?". I start with a stylized example, one-shot prompting GPT-4 with prompts from the THT dataset.
